# Setting this allows creating a symlink to Justfile from another dir
set working-directory := "/home/nicolo/llmd/llm-scripts/llmd"

# Needed for the proxy server
vllm-directory := "/home/nicolo/llmd/vllm/" 

MODEL := "Qwen/Qwen3-0.6B"
# MODEL := "google/gemma-3-1b-it"
# MODEL := "meta-llama/Llama-3.1-8B-Instruct"
# MODEL := "deepseek-ai/DeepSeek-V2-Lite"
# MODEL := "deepseek-ai/deepseek-vl2-small"
# MODEL := "google/gemma-3-1b-it"

# MODEL := "meta-llama/Meta-Llama-3-8B-Instruct"
TP_SIZE := "2"
PREFILL_TP_SIZE := "1"
# should use different ones cause it appears to crash oom when sharing
# though there's plenty of memory
PREFILL_GPUS := "6"
DECODE_GPUS := "6,7"

MEMORY_UTIL := "0.4"
BLOCK_SIZE := "128"
MAX_MODEL_LEN := "1024" # 32768

port PORT: 
  @python port_allocator.py {{PORT}}


regular:
    CUDA_VISIBLE_DEVICES={{PREFILL_GPUS}} \
    VLLM_LOGGING_LEVEL="DEBUG" \
    VLLM_WORKER_MULTIPROC_METHOD=spawn \
    VLLM_ENABLE_V1_MULTIPROCESSING=0 \
    vllm serve {{MODEL}} \
      --port $(just port 8100) \
      --enforce-eager \
      --disable-log-requests \
      --tensor-parallel-size {{PREFILL_TP_SIZE}} \
      --gpu-memory-utilization 1.0 \
      --trust-remote-code \
      --max-model-len 2048 \
      --max-num-seqs 1 \
      --data_parallel_size 1 \
      --max-num-batched-tokens 512 \

    # VLLM_WORKER_MULTIPROC_METHOD=spawn \
    # VLLM_ENABLE_V1_MULTIPROCESSING=0 \

prefill:
  VLLM_NIXL_SIDE_CHANNEL_PORT=$(just port 5557) \
    UCX_LOG_LEVEL=info \
    NIXL_TELEMETRY_ENABLE=y \
    VLLM_ENABLE_V1_MULTIPROCESSING=0 \
    CUDA_VISIBLE_DEVICES={{PREFILL_GPUS}} \
    VLLM_LOGGING_LEVEL="INFO" \
     vllm serve {{MODEL}} \
      --port $(just port 8100) \
      --enforce-eager \
      --disable-log-requests \
      --tensor-parallel-size {{PREFILL_TP_SIZE}} \
      --gpu-memory-utilization {{MEMORY_UTIL}} \
      --trust-remote-code \
      --max-model-len {{MAX_MODEL_LEN}} \
      --block-size {{BLOCK_SIZE}} \
      --data_parallel_size 1 \
      --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
      
  # --speculative_config '{"model": "yuhuili/EAGLE-LLaMA3-Instruct-8B", "num_speculative_tokens": 2, "method": "eagle", "draft_tensor_parallel_size": 1}' \
# --max-num-seqs 1 \
# --max-num-batched-tokens 512 \
decode:
    VLLM_NIXL_SIDE_CHANNEL_PORT=$(just port 5558) \
    UCX_LOG_LEVEL=info \
    NIXL_TELEMETRY_ENABLE=y \
    VLLM_ENABLE_V1_MULTIPROCESSING=0 \
    CUDA_VISIBLE_DEVICES={{DECODE_GPUS}} \
    VLLM_LOGGING_LEVEL="INFO" \
    vllm serve {{MODEL}} \
      --port $(just port 8200) \
      --enforce-eager \
      --disable-log-requests \
      --tensor-parallel-size {{TP_SIZE}} \
      --gpu-memory-utilization {{MEMORY_UTIL}} \
      --trust-remote-code \
      --max-model-len {{MAX_MODEL_LEN}} \
      --block-size {{BLOCK_SIZE}} \
      --data_parallel_size 1 \
      --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'

# --speculative_config '{"model": "yuhuili/EAGLE-LLaMA3-Instruct-8B", "num_speculative_tokens": 2, "method": "eagle", "draft_tensor_parallel_size": 1}' \
# --max-num-seqs 1 \
# --max-num-batched-tokens 512 \
proxy:
    python "{{vllm-directory}}/tests/v1/kv_connector/nixl_integration/toy_proxy_server.py" \
      --port $(just port 8192) \
      --prefiller-port $(just port 8100) \
      --decoder-port $(just port 8200)

send_request:
  curl -X POST http://localhost:$(just port 8192)/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Question: Olivia is organizing a charity event where she plans to hand out goodie bags to guests. She initially prepares 24 bags, each containing 3 small toys, 2 candy bars, and 5 stickers. After checking the guest list again, she realizes that 8 more children are attending than she expected. So she decides to prepare 8 more bags. However, when she goes to the store to get more supplies, she learns that the toys are sold only in packs of 6, candy bars in packs of 4, and stickers in packs of 10. She buys 4 packs of toys, 5 packs of candy bars, and 5 packs of stickers. After returning home, she assembles the remaining 8 bags. Later, she finds out that 3 of the original bags had missing candy bars, so she opens those bags and replaces the missing items. The next morning, she realizes she miscounted the total number of guests, and actually only 26 children are attending, not 32 as she previously thought. She decides to store the extra bags for a future event. How many complete goodie bags does Olivia now have available for the event, assuming each bag must contain exactly 3 toys, 2 candy bars, and 5 stickers? Answer: Question: Olivia is organizing a charity event where she plans to hand out goodie bags to guests. She initially prepares 24 bags, each containing 3 small toys, 2 candy bars, and 5 stickers. After checking the guest list again, she realizes that 8 more children are attending than she expected. So she decides to prepare 8 more bags. However, when she goes to the store to get more supplies, she learns that the toys are sold only in packs of 6, candy bars in packs of 4, and stickers in packs of 10. She buys 4 packs of toys, 5 packs of candy bars, and 5 packs of stickers. After returning home, she assembles the remaining 8 bags. Later, she finds out that 3 of the original bags had missing candy bars, so she opens those bags and replaces the missing items. The next morning, she realizes she miscounted the total number of guests, and actually only 26 children are attending, not 32 as she previously thought. She decides to store the extra bags for a future event. How many complete goodie bags does Olivia now have available for the event, assuming each bag must contain exactly 3 toys, 2 candy bars, and 5 stickers? Answer:", \
      "max_tokens": 150, \
      "temperature": 0.2 \
    }'
send_request2:
  curl -X POST http://localhost:$(just port 8192)/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Can you complete this latin sentence: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.", \
      "max_tokens": 150, \
      "temperature": 0.2 \
    }'
small_req_no_read:
  curl -X POST http://localhost:$(just port 8192)/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Hello how are you", \
      "max_tokens": 50, \
      "temperature": 0.2 \
    }'

send_request_regular:
  curl -X POST http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because ", \
      "max_tokens": 150, \
      "temperature": 0.2 \
    }'

eval:
  lm_eval --model local-completions --tasks gsm8k \
    --model_args model={{MODEL}},base_url=http://127.0.0.1:$(just port 8192)/v1/completions,num_concurrent=5,max_retries=3,tokenized_requests=False \
    --limit 100


# Make sure handhshake isnt accounted
benchmark_one INPUT_LEN:
  python {{vllm-directory}}benchmarks/benchmark_one_concurrent_req.py \
    --model {{MODEL}} \
    --input-len {{INPUT_LEN}} \
    --output-len 1 \
    --num-requests 10 \
    --seed $(date +%s) \
    --port $(just port 8192)