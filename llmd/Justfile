# Setting this allows creating a symlink to Justfile from another dir
set working-directory := "/home/nicolo/vllmd/"

# Needed for the proxy server
vllm-directory := "/home/nicolo/vllmd/vllm/" 

# MODEL := "Qwen/Qwen3-0.6B"
MODEL := "jinaai/ReaderLM-v2"
# MODEL := "meta-llama/Llama-3.1-8B-Instruct"
# MODEL := "deepseek-ai/DeepSeek-V2-Lite"
# MODEL := "deepseek-ai/deepseek-vl2-small"

# MODEL := "meta-llama/Llama-3.1-8B-Instruct"
TP_SIZE := "4"
PREFILL_TP_SIZE := "2"
# PREFILL_GPUS := "0"
# DECODE_GPUS := "1"
# should use different ones cause it appears to crash oom when sharing
# though there's plenty of memory
PREFILL_GPUS := "0,1"
DECODE_GPUS := "4,5,6,7"

MEMORY_UTIL := "0.3"

port PORT: 
  @python port_allocator.py {{PORT}}

# For comparing against baseline vLLM
# vanilla_serve:
#     CUDA_VISIBLE_DEVICES={{PREFILL_GPUS}} \
#     VLLM_LOGGING_LEVEL="DEBUG" \
#     VLLM_WORKER_MULTIPROC_METHOD=spawn \
#     VLLM_ENABLE_V1_MULTIPROCESSING=0 \
#     vllm serve {{MODEL}} \
#       --port $(just port 8192) \
#       --enforce-eager \
#       --disable-log-requests \
#       --tensor-parallel-size {{TP_SIZE}} \
#       --gpu-memory-utilization {{MEMORY_UTIL}} \
#       --trust-remote-code

# prefill:
#     VLLM_NIXL_SIDE_CHANNEL_PORT=$(just port 5557) \
#     UCX_LOG_LEVEL=info \
#     CUDA_VISIBLE_DEVICES={{PREFILL_GPUS}} \
#     VLLM_LOGGING_LEVEL="DEBUG" \
#     VLLM_WORKER_MULTIPROC_METHOD=spawn \
#     VLLM_ENABLE_V1_MULTIPROCESSING=0 \
#     vllm serve {{MODEL}} \
#       --port $(just port 8100) \
#       --enforce-eager \
#       --disable-log-requests \
#       --tensor-parallel-size {{PREFILL_TP_SIZE}} \
#       --gpu-memory-utilization {{MEMORY_UTIL}} \
#       --trust-remote-code \
#       --max-num-seqs 128 \
#       --data_parallel_size 1 \
#       --max-num-batched-tokens 2048 \
#       --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'

# decode:
#     VLLM_NIXL_SIDE_CHANNEL_PORT=$(just port 5558) \
#     UCX_LOG_LEVEL=info \
#     CUDA_VISIBLE_DEVICES={{DECODE_GPUS}} \
#     VLLM_LOGGING_LEVEL="DEBUG" \
#     VLLM_WORKER_MULTIPROC_METHOD=spawn \
#     VLLM_ENABLE_V1_MULTIPROCESSING=0 \
#     vllm serve {{MODEL}} \
#       --port $(just port 8200) \
#       --enforce-eager \
#       --disable-log-requests \
#       --tensor-parallel-size {{TP_SIZE}} \
#       --gpu-memory-utilization {{MEMORY_UTIL}} \
#       --trust-remote-code \
#       --max-num-seqs 128 \
#       --data_parallel_size 1 \
#       --max-num-batched-tokens 2048 \
#       --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
regular:
    CUDA_VISIBLE_DEVICES={{PREFILL_GPUS}} \
    VLLM_LOGGING_LEVEL="DEBUG" \
    VLLM_WORKER_MULTIPROC_METHOD=spawn \
    VLLM_ENABLE_V1_MULTIPROCESSING=0 \
    vllm serve {{MODEL}} \
      --port $(just port 8100) \
      --enforce-eager \
      --disable-log-requests \
      --tensor-parallel-size {{PREFILL_TP_SIZE}} \
      --gpu-memory-utilization 1.0 \
      --trust-remote-code \
      --max-model-len 2048 \
      --max-num-seqs 1 \
      --data_parallel_size 1 \
      --max-num-batched-tokens 512 \

prefill:
    VLLM_NIXL_SIDE_CHANNEL_PORT=$(just port 5557) \
    UCX_LOG_LEVEL=info \
    CUDA_VISIBLE_DEVICES={{PREFILL_GPUS}} \
    VLLM_LOGGING_LEVEL="DEBUG" \
    VLLM_WORKER_MULTIPROC_METHOD=spawn \
    VLLM_ENABLE_V1_MULTIPROCESSING=0 \
    vllm serve {{MODEL}} \
      --port $(just port 8100) \
      --enforce-eager \
      --disable-log-requests \
      --tensor-parallel-size {{PREFILL_TP_SIZE}} \
      --gpu-memory-utilization {{MEMORY_UTIL}} \
      --trust-remote-code \
      --max-model-len 2048 \
      --max-num-seqs 1 \
      --data_parallel_size 1 \
      --max-num-batched-tokens 512 \
      --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'

decode:
    VLLM_NIXL_SIDE_CHANNEL_PORT=$(just port 5558) \
    UCX_LOG_LEVEL=info \
    CUDA_VISIBLE_DEVICES={{DECODE_GPUS}} \
    VLLM_LOGGING_LEVEL="DEBUG" \
    VLLM_WORKER_MULTIPROC_METHOD=spawn \
    VLLM_ENABLE_V1_MULTIPROCESSING=0 \
    vllm serve {{MODEL}} \
      --port $(just port 8200) \
      --enforce-eager \
      --disable-log-requests \
      --tensor-parallel-size {{TP_SIZE}} \
      --gpu-memory-utilization {{MEMORY_UTIL}} \
      --trust-remote-code \
      --max-model-len 2048 \
      --max-num-seqs 1 \
      --data_parallel_size 1 \
      --max-num-batched-tokens 512 \
      --kv-transfer-config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'

proxy:
    python "{{vllm-directory}}/tests/v1/kv_connector/nixl_integration/toy_proxy_server.py" \
      --port $(just port 8192) \
      --prefiller-port $(just port 8100) \
      --decoder-port $(just port 8200)

send_request:
  curl -X POST http://localhost:$(just port 8192)/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Question: Olivia is organizing a charity event where she plans to hand out goodie bags to guests. She initially prepares 24 bags, each containing 3 small toys, 2 candy bars, and 5 stickers. After checking the guest list again, she realizes that 8 more children are attending than she expected. So she decides to prepare 8 more bags. However, when she goes to the store to get more supplies, she learns that the toys are sold only in packs of 6, candy bars in packs of 4, and stickers in packs of 10. She buys 4 packs of toys, 5 packs of candy bars, and 5 packs of stickers. After returning home, she assembles the remaining 8 bags. Later, she finds out that 3 of the original bags had missing candy bars, so she opens those bags and replaces the missing items. The next morning, she realizes she miscounted the total number of guests, and actually only 26 children are attending, not 32 as she previously thought. She decides to store the extra bags for a future event. How many complete goodie bags does Olivia now have available for the event, assuming each bag must contain exactly 3 toys, 2 candy bars, and 5 stickers? Answer: Question: Olivia is organizing a charity event where she plans to hand out goodie bags to guests. She initially prepares 24 bags, each containing 3 small toys, 2 candy bars, and 5 stickers. After checking the guest list again, she realizes that 8 more children are attending than she expected. So she decides to prepare 8 more bags. However, when she goes to the store to get more supplies, she learns that the toys are sold only in packs of 6, candy bars in packs of 4, and stickers in packs of 10. She buys 4 packs of toys, 5 packs of candy bars, and 5 packs of stickers. After returning home, she assembles the remaining 8 bags. Later, she finds out that 3 of the original bags had missing candy bars, so she opens those bags and replaces the missing items. The next morning, she realizes she miscounted the total number of guests, and actually only 26 children are attending, not 32 as she previously thought. She decides to store the extra bags for a future event. How many complete goodie bags does Olivia now have available for the event, assuming each bag must contain exactly 3 toys, 2 candy bars, and 5 stickers? Answer:", \
      "max_tokens": 150, \
      "temperature": 0.2 \
    }'
small_req_no_read:
  curl -X POST http://localhost:$(just port 8192)/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Hello how are you", \
      "max_tokens": 50, \
      "temperature": 0.2 \
    }'

send_request_regular:
  curl -X POST http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{ \
      "model": "{{MODEL}}", \
      "prompt": "Red Hat is the best open source company by far across Linux, K8s, and AI, and vLLM has the greatest community in open source AI software infrastructure. I love vLLM because ", \
      "max_tokens": 150, \
      "temperature": 0.2 \
    }'

eval:
  lm_eval --model local-completions --tasks gsm8k \
    --model_args model={{MODEL}},base_url=http://127.0.0.1:$(just port 8192)/v1/completions,num_concurrent=5,max_retries=3,tokenized_requests=False \
    --limit 100
